{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e923985-5ab0-4fa1-a2ce-93c1284dcce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,from_json\n",
    "import time\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4088f9d6-da3d-401d-bf1d-7baa19354d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSparkStreamingExample\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.4\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2ac9d17-39bc-4600-9145-1a560bdb90b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "                StructField(\"name\", StringType()),\n",
    "                StructField(\"location\", StringType()),\n",
    "                StructField(\"phone\", StringType()),\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0846a4f9-75a1-4237-98c7-1dd2da8bca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuración de lectura desde Kafka\n",
    "kafka_bootstrap_servers = \"kafka:9092\"  # Cambia al host y puerto de tu cluster Kafka\n",
    "kafka_topic = \"test\"  # Nombre del tópico de Kafka\n",
    "\n",
    "kafkaParams = {\n",
    "    \"kafka.bootstrap.servers\": f\"{kafka_bootstrap_servers}\",  # Replace with your broker addresses\n",
    "    \"subscribe\":f\"{kafka_topic}\",  # Replace with your topic name\n",
    "    \"startingOffsets\": \"earliest\",  # or \"earliest\"\n",
    "    \"key.deserializer\": \"org.apache.kafka.common.serialization.StringDeserializer\",\n",
    "    \"value.deserializer\": \"org.apache.kafka.common.serialization.StringDeserializer\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d722df9b-4e90-4b95-ad51-e6384039df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer datos de Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**kafkaParams)\\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc8089b-4252-4df2-b5a6-042c835ec3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = kafka_df.writeStream.format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f00b7ad-d964-4136-ab7a-cf0c3e41e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_events(df_streaming):\n",
    "    status = df_streaming.status['message']\n",
    "    #last_batch_time = time.time()\n",
    "    \n",
    "    while status != 'Stopped':\n",
    "        time.sleep(2)\n",
    "        print(status)\n",
    "        if status == 'Stopped':\n",
    "            df_streaming.stop()\n",
    "    \n",
    "    print(status)\n",
    "#        if status == 'Waiting for data to arrive' and time.time() - last_batch_time > 10:\n",
    "#            df_streaming.stop()\n",
    "#            print('The steam was waiting for data and not recibe 10 seconds ago')\n",
    "#        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9167cd40-e7ae-49c8-8ce3-65fb5ee716df",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwait_events\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m, in \u001b[0;36mwait_events\u001b[0;34m(df_streaming)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#last_batch_time = time.time()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      8\u001b[0m         df_streaming\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wait_events(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae3f8824-4d72-4844-b01f-15d1b6f71842",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc80e2d-4426-4005-b996-18efa1b0f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_df = kafka_df.select(\n",
    "    col(\"key\").cast(\"string\").alias(\"key\"),(col(\"value\").cast(\"string\").alias(\"value\")))\n",
    "\n",
    "messages_df.select(from_json(col('value'), schema).alias(\"data\")).select((\"data.*\"))\n",
    "\n",
    "messages_df\n",
    "# Imprimir los mensajes usando foreachBatch\n",
    "def print_messages(batch_df, batch_id):\n",
    "    print(f\"--- Batch ID: {batch_id} ---\")\n",
    "    for row in batch_df.collect():  # Recorrer las filas del microbatch\n",
    "        print(f\"Key: {row['key']}, Value: {row['value']}\")\n",
    "\n",
    "query = messages_df.writeStream \\\n",
    "    .foreachBatch(messages_df) \\\n",
    "    .trigger(processingTime=\"2 seconds\") \\\n",
    "    .option('checkpointLocation', './spark_kafka_checkpoint')\\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4dc51c-a3be-4967-be88-248f6d9bca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = kafka_df.writeStream.format(\"console\").start()\n",
    "print(query.status)\n",
    "time.sleep(100) # sleep 10 seconds\n",
    "print(query.status)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c256f3d8-79b2-4226-9fd6-f11978e62b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = kafka_df.writeStream.format(\"console\").start()\n",
    "print(query.status)\n",
    "time.sleep(10) # sleep 10 seconds\n",
    "print(query.status)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b4d279-7120-40b2-bfbd-0ec551126e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.status['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f111cc-7262-4e16-af34-6e26291844b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f756b58-9da4-48f6-b294-0e9075c7d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir clave y valor a cadenas\n",
    "messages_df = kafka_df.select(\n",
    "    col(\"key\").cast(\"string\").alias(\"key\"),(col(\"value\").cast(\"string\").alias(\"value\")))\n",
    "\n",
    "messages_df.select(from_json(col('value'), schema).alias(\"data\")).select((\"data.*\"))\n",
    "\n",
    "messages_df\n",
    "# Imprimir los mensajes usando foreachBatch\n",
    "def print_messages(batch_df, batch_id):\n",
    "    print(f\"--- Batch ID: {batch_id} ---\")\n",
    "    for row in batch_df.collect():  # Recorrer las filas del microbatch\n",
    "        print(f\"Key: {row['key']}, Value: {row['value']}\")\n",
    "\n",
    "query = messages_df.writeStream \\\n",
    "    .foreachBatch(messages_df) \\\n",
    "    .trigger(processingTime=\"2 seconds\") \\\n",
    "    .option('checkpointLocation', './spark_kafka_checkpoint')\\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a180bd1d-61ac-4afe-a27f-5e50f0da1c49",
   "metadata": {},
   "source": [
    "# Monitor for inactivity\n",
    "timeout_seconds = 2\n",
    "while query.isActive:\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Stop the query if no messages have been received for the timeout period\n",
    "    if current_time - last_batch_time > timeout_seconds:\n",
    "        print(\"No messages received for 2 seconds. Completing the job...\")\n",
    "        query.stop()\n",
    "\n",
    "# Proceed to the next step\n",
    "print(\"Streaming job completed. Proceeding to the next step...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0013a3bb-cf6e-4b06-84ca-3fd9d32aa307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
